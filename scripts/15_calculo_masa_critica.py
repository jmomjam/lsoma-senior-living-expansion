import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
import os

# --- CONFIGURACI√ìN ---
ARCHIVO_PUNTOS_RAW = "../datos/ranking_fase6_geo_ready.csv"
ARCHIVO_MATRIZ_P = "../datos/matriz_P_nacional_filtrada.parquet"  # Para c√°lculo real de targets
OUTPUT_CLUSTERS = "../datos/ranking_fase8_clusters_analizados.csv"
OUTPUT_PUNTOS_TAGGED = "../datos/ranking_fase8_puntos_con_cluster.csv"

# CONSTANTES F√çSICAS DEL NEGOCIO
COLS_TARGET = ['M_80-84', 'M_85-89', 'M_90-94', 'M_95-99', 'M_100 y m√°s']  # Columnas de mujeres 80+
MARKET_SHARE = 0.03         # Cuota de captura (3%)
UMBRAL_CAMAS = 85           # Break-even operativo

def calcular_masa_critica():
    print("--- SCRIPT 15: C√ÅLCULO DE MASA CR√çTICA Y VIABILIDAD (CORREGIDO) ---")
    
    # 1. CARGAR DATOS CRUDOS
    df = pd.read_csv(ARCHIVO_PUNTOS_RAW, sep=';')
    
    # 1.1 CORRECCI√ìN CR√çTICA: Cargar Matriz P para c√°lculo real de targets
    print("   Cargando Matriz P para c√°lculo real de targets...")
    import os
    if os.path.exists(ARCHIVO_MATRIZ_P):
        df_matriz = pd.read_parquet(ARCHIVO_MATRIZ_P)
        # Calcular % de target (mujeres 80+) por secci√≥n
        df_matriz['Pct_Target'] = df_matriz[COLS_TARGET].sum(axis=1)
        # Mergear con df principal
        df = pd.merge(df, df_matriz[['Pct_Target']], left_on='Seccion', right_index=True, how='left')
        df['Pct_Target'] = df['Pct_Target'].fillna(0)
        # Poblaci√≥n Target Real = Poblaci√≥n Total * % Target
        df['Poblacion_Target_Real'] = df['Poblacion_Total'] * df['Pct_Target']
        print(f"   ‚úî Target promedio real calculado: {df['Poblacion_Target_Real'].mean():.1f} mujeres/secci√≥n")
    else:
        # Fallback: usar estimaci√≥n del 6% de la poblaci√≥n
        print("   ‚ö†Ô∏è Matriz P no encontrada, usando estimaci√≥n (6% poblaci√≥n)")
        df['Poblacion_Target_Real'] = df['Poblacion_Total'] * 0.06
    
    # Filtro de Calidad previo al Clustering (Top 15% Score)
    umbral_score = df['Score_Global'].quantile(0.85)
    df_ml = df[df['Score_Global'] > umbral_score].copy()
    print(f"   Puntos analizados (Top 15%): {len(df_ml)}")

    # 2. GENERAR CLUSTERS (DBSCAN)
    # Re-calculamos aqu√≠ para asegurar que tenemos el ID en cada punto
    # Primero eliminamos filas con coordenadas faltantes (NaN)
    filas_antes = len(df_ml)
    df_ml = df_ml.dropna(subset=['LATITUD', 'LONGITUD'])
    filas_despues = len(df_ml)
    if filas_antes != filas_despues:
        print(f"   ‚ö†Ô∏è Eliminadas {filas_antes - filas_despues} filas sin coordenadas v√°lidas")
    
    if len(df_ml) == 0:
        print("‚ùå No quedan puntos con coordenadas v√°lidas.")
        return
    
    coords = np.radians(df_ml[['LATITUD', 'LONGITUD']].values)
    # Radio 1.5km
    db = DBSCAN(eps=1.5/6371., min_samples=3, metric='haversine', algorithm='ball_tree').fit(coords)
    
    df_ml['Cluster_ID'] = db.labels_
    
    # Descartamos el ruido (-1)
    df_ml = df_ml[df_ml['Cluster_ID'] != -1].copy()

    # 3. AN√ÅLISIS DE MASA CR√çTICA POR CLUSTER
    # Agrupamos para ver las propiedades macrosc√≥picas
    stats = df_ml.groupby('Cluster_ID').agg({
        'Seccion': 'count',                 # N√∫mero de secciones (Volumen)
        'Score_Global': 'mean',             # Calidad media
        'Renta_Hogar': 'mean',
        'Presion_Cuidados': 'mean',
        'LATITUD': 'mean',
        'LONGITUD': 'mean',
        'Poblacion_Target_Real': 'sum'      # SUMA REAL de mujeres 80+ en el cluster
    }).rename(columns={'Seccion': 'Num_Secciones'})
    
    # F√ìRMULA DE VIABILIDAD (CORREGIDA)
    # Capacidad = Suma_Target_Real * Share (ya no usa constante fija)
    stats['Capacidad_Teorica_Camas'] = stats['Poblacion_Target_Real'] * MARKET_SHARE
    
    # ETIQUETADO BINARIO
    stats['Es_Viable'] = stats['Capacidad_Teorica_Camas'] >= UMBRAL_CAMAS
    
    # Estad√≠sticas Globales
    viables = stats[stats['Es_Viable']]
    subcriticos = stats[~stats['Es_Viable']]
    
    camas_totales = viables['Capacidad_Teorica_Camas'].sum()
    residencias_posibles = int(camas_totales / 100)
    
    print("\n--- RESULTADOS DEL C√ÅLCULO ---")
    print(f"   Clusters Totales Detectados: {len(stats)}")
    print(f"   ‚úÖ Clusters VIABLES (>85 camas): {len(viables)}")
    print(f"   ‚ö†Ô∏è Clusters SUB-CR√çTICOS (Descartados): {len(subcriticos)}")
    print(f"   üè≠ Potencial de Construcci√≥n (Solo Viables): {residencias_posibles} Residencias")
    
    if residencias_posibles < 1000:
        deficit = 1000 - residencias_posibles
        print(f"   üìâ D√âFICIT DE OBJETIVO: Faltan {deficit} residencias para llegar a 1000.")
        print("      -> Se recomienda estrategia M&A (Adquisiciones) para cubrir el hueco.")

    # 4. GUARDADO DE DATOS
    # Guardamos el resumen de clusters
    stats.to_csv(OUTPUT_CLUSTERS, sep=';')
    
    # Guardamos los puntos individuales ETIQUETADOS (para poder pintarlos luego)
    # Hacemos merge para pegar la info de viabilidad a cada punto
    df_final_puntos = pd.merge(df_ml, stats[['Es_Viable', 'Capacidad_Teorica_Camas']], 
                               left_on='Cluster_ID', right_index=True)
    
    # Limpiamos el c√≥digo CUSEC para el cruce con Shapefile (quitamos espacios y texto)
    # De "3120104001 Pamplona..." a "3120104001"
    df_final_puntos['CUSEC_LIMPIO'] = df_final_puntos['Seccion'].astype(str).str.split(' ').str[0].str.strip()
    
    df_final_puntos.to_csv(OUTPUT_PUNTOS_TAGGED, sep=';', index=False)
    
    print(f"\n‚úÖ Datos procesados guardados en:")
    print(f"   1. Resumen Clusters: {OUTPUT_CLUSTERS}")
    print(f"   2. Detalle Puntos: {OUTPUT_PUNTOS_TAGGED}")

if __name__ == "__main__":
    calcular_masa_critica()
